# -*- coding: utf-8 -*-
"""CANCER_PREDICTION.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MHh_-ShbnBAl7N32ePlEniwems-WVifc
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import torch

data = pd.read_csv("data_day1.csv")

data.head(20)

data.tail()

data.drop(['id', 'Unnamed: 32'], axis=1, inplace=True)

data

data.shape

data.dtypes

data.head()

data['diagnosis'].value_counts()

#sns.countplot(data=data, x='diagnosis')

#! pip install scikit-learn
from sklearn.preprocessing import LabelEncoder

encoder = LabelEncoder()

data['diagnosis'] = encoder.fit_transform(data['diagnosis'])

data.head()

data.diagnosis.value_counts()

"""B -- > 0, and M -- > 1"""

data.head()

x = data.drop('diagnosis', axis=1)
y = data.diagnosis.values

x.head()

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

x = scaler.fit_transform(x)

x.shape

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2, random_state=42)

x_train.shape

y_train.shape

x_test.shape

y_test.shape

from torch.utils.data import Dataset, DataLoader

class TabularData(Dataset):
  def __init__(self, x,y):
    super().__init__()
    self.x = torch.from_numpy(x).type(torch.float32)
    self.y = torch.from_numpy(y).type(torch.float32)

  def __len__(self):
    return len(self.x)

  def __getitem__(self, id):
    return self.x[id], self.y[id]

train_data = TabularData(x_train, y_train)

train_data[2]

train_loader = DataLoader(train_data, batch_size=8, shuffle=True)

#This code defines a simple binary classifier neural network using PyTorch
# This defines a new neural network model class called BinClr, which inherits from torch.nn.Module (the base class for all neural network modules in PyTorch).

class BinClr(torch.nn.Module):
  def __init__(self):
    super().__init__()

    self.layer1 = torch.nn.Linear(in_features=30, out_features=32) # input layer
    self.layer2 = torch.nn.Linear(in_features=32, out_features=16) # hidden layer
    self.layer3 = torch.nn.Linear(in_features= 16, out_features=1) # output layer
    self.relu = torch.nn.ReLU() # Activation function: Applies ReLU (Rectified Linear Unit), which introduces non-linearity.

  def forward(self, x : torch.Tensor):  #forward Method – Data Flow:how x is passed through network to output

    return self.layer3(self.relu(self.layer2(self.relu(self.layer1(x)))))
#This defines how input x passes through the network:
# Input x is passed to layer1: shape [batch_size, 30] → [batch_size, 32]
# Apply ReLU
# Pass through layer2: [batch_size, 32] → [batch_size, 16]
# Apply ReLU again
# Pass through layer3: [batch_size, 16] → [batch_size, 1]
# Output: a single value per sample (logit for binary classification)

# # This code defines a simple binary classification neural network using PyTorch, where the model takes 30 input features
# and passes them through two hidden layers (with 32 and 16 neurons) using ReLU activations, and finally outputs a single value (logit) for prediction.
# The layers are organized using `torch.nn.Sequential`, which makes the architecture cleaner and easier to manage.
# The `forward` method simply passes the input through this sequence of layers.
#  It's typically used with a sigmoid activation during inference and trained using a binary classification loss like `BCEWithLogitsLoss`.
class BinClr(torch.nn.Module):
  def __init__(self):
    super().__init__()
    self.order = torch.nn.Sequential(
        torch.nn.Linear(in_features=30, out_features=32),
        torch.nn.ReLU(),
        torch.nn.Linear(in_features=32, out_features=16),
        torch.nn.ReLU(),
        torch.nn.Linear(in_features=16, out_features=1)
    )

  def forward(self, x : torch.Tensor):
    return self.order(x)

device = 'cuda' if torch.cuda.is_available() else 'cpu'

device

model = BinClr().to(device)

"""ReLU stands for Rectified Linear Unit and is a widely used activation function in neural networks. It's defined as f(x) = max(0, x), meaning it outputs the input if it's positive, and zero otherwise. This function introduces non-linearity into neural networks, enabling them to learn complex patterns.

ReLU's "Switch":
ReLU acts like a switch. For positive inputs, it passes them through without change. For negative inputs, it switches them off by setting them to zero. This "switch" introduces a non-linear decision boundary.
Why is this important?:
Without non-linearity, a neural network with multiple layers would essentially act like a single, large linear model. ReLU helps the network learn complex relationships and make more sophisticated predictions.
model
"""

model.state_dict() # display for the columns with weight value

#This line of code takes a batch of data, uses your model to predict the probabilities of the data belonging to class 1,
# applies a threshold of 0.5 to convert probabilities into binary predictions (0 or 1), and then represents those predictions as integers.
(torch.sigmoid(model(next(iter(train_loader))[0].type(torch.float32)))>=0.5).type(torch.int)  # saving with float datatype

loss = torch.nn.BCELoss()
optim = torch.optim.Adam(params = model.parameters(), lr = 0.001)

#This code trains a model for 15 rounds (called **epochs**).
# In each epoch, it goes through all the training data in small parts (called **batches**). For each batch, it does 4 main steps:
# 1. **Predicts** the output using the model.
# 2. **Calculates the error** (loss) by comparing the prediction with the actual answer.
# 3. **Updates the model** by adjusting weights to reduce the error using backpropagation.
# 4. Keeps track of the loss for the whole epoch.
# At the end of each epoch, it prints the average loss to show how well the model is learning over time.


epochs = 15 # accuarcy is based on this value

for epoch in range(epochs):
  temp_loss = []
  for i, (features, target) in enumerate(train_loader):
    model.train()

    y_pred = torch.sigmoid(model(features))

    loss_fn = loss(y_pred.squeeze(), target)

    temp_loss.append(loss_fn.item())

    optim.zero_grad()

    loss_fn.backward()

    optim.step()

  print(f"Epoch {epoch} | loss: {sum(temp_loss)/len(temp_loss)}")

# #accuracy_score
# Tells you how many predictions were correct — like a test score.
# If your model got 8 out of 10 right, accuracy is 0.8 (or 80%).
#confusion_matrix
# Gives a table that shows:
# How many times the model got 1s right (True Positives)
# Got 0s right (True Negatives)
# Mistakenly predicted 1 instead of 0 (False Positives)
# Mistakenly predicted 0 instead of 1 (False Negatives)
from sklearn.metrics import accuracy_score, confusion_matrix

x_test.shape # 20 % of test data

y_test

#It gives you the model’s predictions for the test data, as probabilities.
#Converts the result back into a NumPy array so you can use it for checking accuracy or other metrics.
y_pred = torch.sigmoid(model(torch.from_numpy(x_test).type(torch.float32))).detach().numpy()

y_pred = (y_pred>=0.5).astype(int).squeeze()

score = 100*(accuracy_score(y_test, y_pred))

score

sns.heatmap(confusion_matrix(y_test, y_pred), fmt='.1f', annot=True)

#predictive system

patient1 = x_train[1]

result = (torch.sigmoid(model(torch.from_numpy(patient1).type(torch.float32)))>=0.5).type(torch.int)[0]

if result == 0:
  print("It is Benign")
else:
  print("It is Malignant")